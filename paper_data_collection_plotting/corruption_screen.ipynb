{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from numba import jit\n",
    "import pickle\n",
    "import utilities as ut\n",
    "import sklearn.metrics as met\n",
    "from SAM import SAM\n",
    "import utilities_full as ut2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graph_tool.all import Graph, local_clustering\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def scatter(self, projection=None, c=None, cmap='rainbow', linewidth=0.0,\n",
    "            edgecolor='k', axes=None, colorbar=True, s=10, **kwargs):\n",
    "    PLOTTING=True\n",
    "    if (not PLOTTING):\n",
    "        print(\"matplotlib not installed!\")\n",
    "    else:\n",
    "        if(isinstance(projection, str)):\n",
    "            try:\n",
    "                dt = self.adata.obsm[projection]\n",
    "            except KeyError:\n",
    "                print('Please create a projection first using run_umap or'\n",
    "                      'run_tsne')\n",
    "\n",
    "        elif(projection is None):\n",
    "            try:\n",
    "                dt = self.adata.obsm['X_umap']\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    dt = self.adata.obsm['X_tsne']\n",
    "                except KeyError:\n",
    "                    print(\"Please create either a t-SNE or UMAP projection\"\n",
    "                          \"first.\")\n",
    "                    return\n",
    "        else:\n",
    "            dt = projection\n",
    "\n",
    "        if(axes is None):\n",
    "            plt.figure()\n",
    "            axes = plt.gca()\n",
    "\n",
    "        if(c is None):\n",
    "            plt.scatter(dt[:, 0], dt[:, 1], s=s,\n",
    "                        linewidth=linewidth, edgecolor=edgecolor, **kwargs)\n",
    "        else:\n",
    "\n",
    "            if isinstance(c, str):\n",
    "                try:\n",
    "                    c = self.adata.obs[c].get_values()\n",
    "                except KeyError:\n",
    "                    0  # do nothing\n",
    "\n",
    "            if((isinstance(c[0], str) or isinstance(c[0], np.str_)) and\n",
    "               (isinstance(c, np.ndarray) or isinstance(c, list))):\n",
    "                i = ut.convert_annotations(c)\n",
    "                ui, ai = np.unique(i, return_index=True)\n",
    "                cax = axes.scatter(dt[:,0], dt[:,1], c=i, cmap=cmap, s=s,\n",
    "                                   linewidth=linewidth,\n",
    "                                   edgecolor=edgecolor,\n",
    "                                   **kwargs)\n",
    "\n",
    "                if(colorbar):\n",
    "                    cbar = plt.colorbar(cax, ax=axes, ticks=ui)\n",
    "                    cbar.ax.set_yticklabels(c[ai])\n",
    "            else:\n",
    "                if not (isinstance(c, np.ndarray) or isinstance(c, list)):\n",
    "                    colorbar = False\n",
    "                i = c\n",
    "\n",
    "                cax = axes.scatter(dt[:,0], dt[:,1], c=i, cmap=cmap, s=s,\n",
    "                                   linewidth=linewidth,\n",
    "                                   edgecolor=edgecolor,\n",
    "                                   **kwargs)\n",
    "\n",
    "                if(colorbar):\n",
    "                    plt.colorbar(cax, ax=axes)\n",
    "def DARMANIS(**kwargs):\n",
    "    sam = SAM()\n",
    "    sam.load_data('darmanis/darmanis_data.csv')\n",
    "    sam.load_obs_annotations('darmanis/darmanis_ann.csv')\n",
    "    sam.preprocess_data(**kwargs)\n",
    "    return sam\n",
    "\n",
    "def WANG(**kwargs):\n",
    "    sam = SAM()\n",
    "    sam.load_data('final_datasets/GSE83139/wang_data_sparse.p', **kwargs)\n",
    "    sam.preprocess_data(**kwargs)\n",
    "    A = pd.read_csv('final_datasets/GSE83139/wang_ann.csv',header=None,index_col=0)    \n",
    "    A.index = A.index.astype(\"<U100\")\n",
    "    sam.adata.obs['ann'] = A\n",
    "    sam.adata.var_names_make_unique()\n",
    "    return sam\n",
    "\n",
    "def human1(**kwargs):\n",
    "    sam = SAM()\n",
    "    sam.load_data('final_datasets/GSE84133_1/human1_sparse.p')\n",
    "    sam.preprocess_data(**kwargs)\n",
    "    sam.load_obs_annotations('final_datasets/GSE84133_1/human1_ann.csv')\n",
    "    return sam\n",
    "\n",
    "\n",
    "def human2(**kwargs):\n",
    "    sam = SAM()\n",
    "    sam.load_data('final_datasets/GSE84133_2/human2_sparse.p')\n",
    "    sam.preprocess_data(**kwargs)\n",
    "    sam.load_obs_annotations('final_datasets/GSE84133_2/human2_ann.csv')\n",
    "    return sam\n",
    "\n",
    "def human3(**kwargs):\n",
    "    sam = SAM()\n",
    "    sam.load_data('final_datasets/GSE84133_3/human3_sparse.p')\n",
    "    sam.preprocess_data(**kwargs)\n",
    "    sam.load_obs_annotations('final_datasets/GSE84133_3/human3_ann.csv')\n",
    "    return sam\n",
    "def human4(**kwargs):\n",
    "    sam = SAM()\n",
    "    sam.load_data('final_datasets/GSE84133_4/human4_sparse.p')\n",
    "    sam.preprocess_data(**kwargs)\n",
    "    sam.load_obs_annotations('final_datasets/GSE84133_4/human4_ann.csv')\n",
    "    return sam\n",
    "\n",
    "\n",
    "def KOH(**kwargs):\n",
    "    sam = SAM()\n",
    "    sam.load_data('final_datasets/SRP073808/SRP073808_data.csv')\n",
    "    sam.load_obs_annotations('final_datasets/SRP073808/SRP073808_ann.csv')\n",
    "    sam.preprocess_data(**kwargs)\n",
    "    return sam\n",
    "\n",
    "\n",
    "def SEGER(**kwargs):\n",
    "    sam=SAM()\n",
    "    sam.load_data('final_datasets/seger/seger_sparse.p')\n",
    "    sam.load_obs_annotations('final_datasets/seger/seger_ann.csv')\n",
    "    sam.preprocess_data(**kwargs)\n",
    "    return sam\n",
    "\n",
    "\n",
    "def MURARO(**kwargs):\n",
    "    sam=SAM()\n",
    "    sam.load_data('final_datasets/muraro/muraro_sparse.p')\n",
    "    sam.load_obs_annotations('final_datasets/muraro/muraro_ann.csv')\n",
    "    sam.preprocess_data(**kwargs)\n",
    "    return sam\n",
    "def nmi(x, y):\n",
    "    return met.adjusted_mutual_info_score(x, y, average_method='arithmetic')\n",
    "\n",
    "\n",
    "def ari(x, y):\n",
    "    return met.adjusted_rand_score(x, y)\n",
    "def SEURAT(adata,npcs,ngenes):\n",
    "    pca,_,_,_ = ut2.do_SEURAT4(adata.copy(),npcs=npcs,NN=ngenes)\n",
    "    cl = hdbknn(pca)\n",
    "    return pca,cl\n",
    "    #RECORD['('+str(npcs)+','+str(ngenes)+')'] = cl\n",
    "def hdbknn(X):\n",
    "    import hdbscan\n",
    "    k=20\n",
    "\n",
    "    hdb = hdbscan.HDBSCAN(metric='euclidean')\n",
    "\n",
    "    cl = hdb.fit_predict(X)\n",
    "\n",
    "    idx0 = np.where(cl != -1)[0]\n",
    "    idx1 = np.where(cl == -1)[0]\n",
    "    if idx1.size > 0 and idx0.size > 0:\n",
    "        xcmap = ut.generate_euclidean_map(X[idx0, :], X[idx1, :])\n",
    "        knn = np.argsort(xcmap.T, axis=1)[:, :k]\n",
    "        nnm = np.zeros(xcmap.shape).T\n",
    "        nnm[np.tile(np.arange(knn.shape[0])[:, None],\n",
    "                    (1, knn.shape[1])).flatten(),\n",
    "            knn.flatten()] = 1\n",
    "        nnmc = np.zeros((nnm.shape[0], cl.max() + 1))\n",
    "        for i in range(cl.max() + 1):\n",
    "            nnmc[:, i] = nnm[:, cl[idx0] == i].sum(1)\n",
    "\n",
    "        cl[idx1] = np.argmax(nnmc, axis=1)\n",
    "\n",
    "    return cl\n",
    "@jit(nopython=True)\n",
    "def permute(D, npermutes):\n",
    "    for i in range(npermutes):\n",
    "        x1, y1 = np.random.randint(0, D.shape[0]), np.random.randint(\n",
    "            0, D.shape[1])\n",
    "        x2, y2 = np.random.randint(0, D.shape[0]), np.random.randint(\n",
    "            0, D.shape[1])\n",
    "        num = D[x1, y1]\n",
    "        D[x1, y1] = D[x2, y2]\n",
    "        D[x2, y2] = num\n",
    "        \n",
    "def permute2(D, n_elements):\n",
    "    x,y = np.unravel_index(np.random.choice(D.size,size=n_elements,replace=False),D.shape)\n",
    "    ind = np.random.permutation(x.size)\n",
    "    D[x[ind],y[ind]] = D[x,y]\n",
    "    \n",
    "def modularity(graph, cl):\n",
    "    indegree = graph.sum(0).flatten()\n",
    "    outdegree = graph.sum(1).flatten()\n",
    "    m = graph.sum()\n",
    "\n",
    "    C = np.zeros(graph.shape)\n",
    "    for i in range(cl.max() + 1):\n",
    "        idxs = np.where(cl == i)[0]\n",
    "        idxs1 = np.repeat(idxs, idxs.size)\n",
    "        idxs2 = np.tile(idxs, idxs.size)\n",
    "        C[idxs1, idxs2] = 1\n",
    "\n",
    "    Q = ((graph - indegree[:, None] * outdegree[None, :] / m) * C / m).sum()\n",
    "    return Q\n",
    "\n",
    "\n",
    "def generate_graph(graph):\n",
    "    Npca = graph.copy()\n",
    "    G = Graph(directed=True)\n",
    "\n",
    "    Npca[np.arange(Npca.shape[0]), np.arange(Npca.shape[0])] = 0\n",
    "\n",
    "    G.add_edge_list(np.transpose(Npca.nonzero()))\n",
    "    return G\n",
    "\n",
    "\n",
    "def local_clust(graph):\n",
    "    G = generate_graph(graph)\n",
    "    return np.mean(list(local_clustering(G, undirected=False)))\n",
    "\n",
    "def l2disp(data, graph, k, N):\n",
    "    davg = graph.dot(data / k)\n",
    "    mu = davg.mean(0)\n",
    "\n",
    "    disp = davg.var(0)[mu > 0] / mu[mu > 0]\n",
    "    disp[np.isnan(disp)] = 0\n",
    "    return np.sqrt(np.sum(np.sort(disp)[-N:]**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = ['Normalizer','Normalizer']+['StandardScaler',]*4+['Normalizer',]*3\n",
    "d = dict(min_expression=1,filter_genes=False)\n",
    "funcs = [DARMANIS(**d),WANG(**d),human1(**d),human2(**d),human3(**d),human4(**d),KOH(**d),SEGER(**d),MURARO(**d)]\n",
    "names = ['DARMANIS','WANG','human1','human2','human3','human4','KOH','SEGER','MURARO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngenes = [500,1000,1500,2000,2500,3000,3500,4000,4500,5000,5500,6000,6500,7000,-1]\n",
    "npcs = [6,10,15,20,25,30,35,40,45,50]\n",
    "RECORDd = pickle.load(open('paper_scripts/seurat_param_sweep_bigger_fixed.p','rb'))\n",
    "nge=[]\n",
    "npc=[]\n",
    "for i in names:\n",
    "    ind = np.where(RECORDd[i]>= RECORDd[i].max())\n",
    "    n1,n2 = ngenes[ind[0][0]],npcs[ind[1][0]]\n",
    "    nge.append(n1)\n",
    "    npc.append(n2)\n",
    "    \n",
    "nge[1]=3000\n",
    "npc[1]=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "ARIsamL=[]\n",
    "ARIseurR=[]\n",
    "ARIseurL=[]\n",
    "ARIseurO=[]\n",
    "for i in range(len(funcs)):\n",
    "    sam = copy.deepcopy(funcs[i])\n",
    "    D = sam.adata_raw.X.copy()\n",
    "    \n",
    "    NTRIALS=50\n",
    "    NREPLICATES=10\n",
    "    \n",
    "    AMIsam = np.zeros((NREPLICATES,NTRIALS))    \n",
    "    AMIseur = np.zeros((NREPLICATES,NTRIALS))\n",
    "    AMIseuro = np.zeros((NREPLICATES,NTRIALS))\n",
    "    AMIseurr = np.zeros((NREPLICATES,NTRIALS))\n",
    "    \n",
    "    ARIsam = np.zeros((NREPLICATES,NTRIALS))    \n",
    "    ARIseur = np.zeros((NREPLICATES,NTRIALS))    \n",
    "    ARIseuro = np.zeros((NREPLICATES,NTRIALS)) \n",
    "    ARIseurr = np.zeros((NREPLICATES,NTRIALS)) \n",
    "\n",
    "\n",
    "    AMIsaml = np.zeros((NREPLICATES,NTRIALS))    \n",
    "    AMIseurl = np.zeros((NREPLICATES,NTRIALS))\n",
    "    AMIseurlo = np.zeros((NREPLICATES,NTRIALS))    \n",
    "    AMIseurlr = np.zeros((NREPLICATES,NTRIALS))    \n",
    "\n",
    "    ARIsaml = np.zeros((NREPLICATES,NTRIALS))    \n",
    "    ARIseurl = np.zeros((NREPLICATES,NTRIALS))       \n",
    "    ARIseurlo = np.zeros((NREPLICATES,NTRIALS))\n",
    "    ARIseurlr = np.zeros((NREPLICATES,NTRIALS))\n",
    "    \n",
    "    METRICSsam = np.zeros((3,NREPLICATES,NTRIALS))    \n",
    "    METRICSseur = np.zeros((3,NREPLICATES,NTRIALS)) \n",
    "    METRICSseuro = np.zeros((3,NREPLICATES,NTRIALS)) \n",
    "    METRICSseurr = np.zeros((3,NREPLICATES,NTRIALS)) \n",
    "    \n",
    "    stds = np.round(np.linspace(0,np.prod(sam.adata.shape),NTRIALS)).astype('int64')\n",
    "    ann = sam.adata.obs.iloc[:,0].get_values()\n",
    "    \n",
    "    sam.k=20\n",
    "    \n",
    "    for j in range(NTRIALS): \n",
    "        for k in range(NREPLICATES):\n",
    "            print(str(i) + ' --- ' + str (j) + ' --- ' + str(k))\n",
    "            Ds = D.A.copy()\n",
    "            permute2(Ds,stds[j])\n",
    "            sam.adata.X=sp.sparse.csr_matrix(Ds)\n",
    "            sam.adata.X.data[:] = np.log2(sam.adata.X.data+1)\n",
    "            sam.adata.X.data[sam.adata.X.data < 1] = 0\n",
    "            sam.adata.X.eliminate_zeros()\n",
    "            \n",
    "            #\"\"\"\n",
    "            sam.adata.layers['X_disp'] = sam.adata.X            \n",
    "            sam.run(stopping_condition=5e-3,preprocessing=preproc[i],projection=None)\n",
    "            sam.hdbknn_clustering(npcs=15)            \n",
    "            sam.louvain_clustering()\n",
    "            sam_hdb = sam.adata.obs['hdbscan_clusters'].get_values()\n",
    "            sam_louv = sam.adata.obs['louvain_clusters'].get_values()\n",
    "            \n",
    "                        \n",
    "            AMIsam[k,j] = nmi(sam_hdb,ann)\n",
    "            ARIsam[k,j] = ari(sam_hdb,ann)\n",
    "            AMIsaml[k,j] = nmi(sam_louv,ann)\n",
    "            ARIsaml[k,j] = ari(sam_louv,ann)     \n",
    "\n",
    "            print('ARIsam: ' + str(ARIsam[k,j]))   \n",
    "            print('ARIsaml: ' + str(ARIsaml[k,j]))   \n",
    "            \n",
    "            X=sam.adata.uns['neighbors']['connectivities'].A            \n",
    "            clx = sam.louvain_clustering(X=X)\n",
    "            Qx = modularity(X, clx)\n",
    "            \n",
    "            METRICSsam[0,k,j] = local_clust(X)\n",
    "            METRICSsam[1,k,j] = Qx\n",
    "            METRICSsam[2,k,j] = l2disp(sam.adata.X.toarray(),X,sam.k,100)   \n",
    "            print('METRICS sam: ' + str(METRICSsam[:,k,j]))                        \n",
    "                        \n",
    "            #\"\"\"\n",
    "            #\"\"\"\n",
    "            adata = sam.adata.copy() # put raw, unfiltered expressions in for seurat\n",
    "            adata.X = sp.sparse.csr_matrix(Ds)\n",
    "            if i == 1:\n",
    "                adata.X[adata.X<1]=0\n",
    "                adata.X.eliminate_zeros()\n",
    "            #adata.X.data[:] = 2**adata.X.data-1\n",
    "                        \n",
    "            \"\"\" L \"\"\"\n",
    "            pca,_,_,seur_louv = ut2.do_SEURAT4(adata.copy(),npcs=15,NN=3000)\n",
    "            seur_hdb = hdbknn(Normalizer().fit_transform(pca))\n",
    "            ARIseur[k,j] = ari(seur_hdb,ann)\n",
    "            AMIseur[k,j] = nmi(seur_hdb,ann)\n",
    "            AMIseurl[k,j] = nmi(seur_louv,ann)\n",
    "            ARIseurl[k,j] = ari(seur_louv,ann)\n",
    "\n",
    "            print('ARIseur: ' + str(ARIseur[k,j]))\n",
    "            print('ARIseurl: ' + str(ARIseurl[k,j]))\n",
    "\n",
    "            Y = ut.dist_to_nn(ut.compute_distances(pca,'correlation'),sam.k)            \n",
    "            cly = sam.louvain_clustering(X=Y)\n",
    "            Qy = modularity(Y, cly)\n",
    "\n",
    "            \n",
    "            METRICSseur[0,k,j] = local_clust(Y)\n",
    "            METRICSseur[1,k,j] = Qy\n",
    "            METRICSseur[2,k,j] = l2disp(sam.adata.X.toarray(),Y,sam.k,100)\n",
    "            \n",
    "            print('METRICS seur: ' + str(METRICSseur[:,k,j]))     \n",
    "            \n",
    "            \"\"\" O \"\"\"\n",
    "            pca,_,_,seur_louv = ut2.do_SEURAT4(adata.copy(),npcs=npc[i],NN=nge[i])\n",
    "            seur_hdb = hdbknn(Normalizer().fit_transform(pca))\n",
    "                \n",
    "            AMIseuro[k,j] = nmi(seur_hdb,ann)\n",
    "            ARIseuro[k,j] = ari(seur_hdb,ann)\n",
    "            AMIseurlo[k,j] = nmi(seur_louv,ann)\n",
    "            ARIseurlo[k,j] = ari(seur_louv,ann)\n",
    "            \n",
    "            print('ARIseurO: ' + str(ARIseuro[k,j]))\n",
    "            print('ARIseurlO: ' + str(ARIseurlo[k,j]))\n",
    "\n",
    "            Y = ut.dist_to_nn(ut.compute_distances(pca,'correlation'),sam.k)\n",
    "            cly = sam.louvain_clustering(X=Y)\n",
    "            Qy = modularity(Y, cly)\n",
    " \n",
    "            METRICSseuro[0,k,j] = local_clust(Y)\n",
    "            METRICSseuro[1,k,j] = Qy\n",
    "            METRICSseuro[2,k,j] = l2disp(sam.adata.X.toarray(),Y,sam.k,100)\n",
    "            print('METRICS seurO: ' + str(METRICSseuro[:,k,j]))             \n",
    "            #\"\"\"\n",
    "            \"\"\" RESCUE \"\"\"\n",
    "            pca,_,_,seur_louv = ut2.do_SEURAT4(adata.copy(),npcs=15,genes = sam.adata.uns['ranked_genes'][:3000])\n",
    "            seur_hdb = hdbknn(Normalizer().fit_transform(pca))\n",
    "            ARIseurr[k,j] = ari(seur_hdb,ann)\n",
    "            AMIseurr[k,j] = nmi(seur_hdb,ann)\n",
    "            AMIseurlr[k,j] = nmi(seur_louv,ann)\n",
    "            ARIseurlr[k,j] = ari(seur_louv,ann)\n",
    "\n",
    "            print('ARIseurR: ' + str(ARIseurr[k,j]))\n",
    "            print('ARIseurlR: ' + str(ARIseurlr[k,j]))\n",
    "\n",
    "            Y = ut.dist_to_nn(ut.compute_distances(pca,'correlation'),sam.k)            \n",
    "            cly = sam.louvain_clustering(X=Y)\n",
    "            Qy = modularity(Y, cly)\n",
    "            \n",
    "            METRICSseurr[0,k,j] = local_clust(Y)\n",
    "            METRICSseurr[1,k,j] = Qy\n",
    "            METRICSseurr[2,k,j] = l2disp(sam.adata.X.toarray(),Y,sam.k,100)\n",
    "            \n",
    "            print('METRICS seurR: ' + str(METRICSseurr[:,k,j]))\n",
    "\n",
    "\n",
    "    ARIsamL.append((AMIsam,ARIsam,AMIsaml,ARIsaml,METRICSsam))\n",
    "    ARIseurL.append((AMIseur,ARIseur,AMIseurl,ARIseurl,METRICSseur))\n",
    "    ARIseurR.append((AMIseurr,ARIseurr,AMIseurlr,ARIseurlr,METRICSseurr))\n",
    "    ARIseurO.append((AMIseuro,ARIseuro,AMIseurlo,ARIseurlo,METRICSseuro))\n",
    "    pickle.dump((ARIsamL,ARIseurL,ARIseurR,ARIseurO),open('paper_scripts/8_23_2019_all_permute2.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mm2inch(*tupl):\n",
    "    inch = 25.4\n",
    "    if isinstance(tupl[0], tuple):\n",
    "        return tuple(i/inch for i in tupl[0])\n",
    "    else:\n",
    "        return tuple(i/inch for i in tupl)\n",
    "    \n",
    "#ARIseurR = pickle.load(open('paper_scripts/8_17_2019_seurat_rescued.p','rb'))\n",
    "#ARIseurL,ARIseurO = pickle.load(open('paper_scripts/to_100p_elife_rev_w_optimize_fixed.p','rb'))\n",
    "#ARIsamL = pickle.load(open('paper_scripts/8_17_elife_rev_corruption.p','rb'))\n",
    "ARIsamL,ARIseurL,ARIseurR,ARIseurO = pickle.load(open('paper_scripts/8_23_2019_all_permute2.p','rb'))\n",
    "\n",
    "name = ['Darmanis','Wang','Baron1','Baron2','Baron3','Baron4','Koh','Segerstolpe','Muraro']#ARI.index[::-1].values\n",
    "AUC1 = np.zeros((10,len(name),4))\n",
    "AUC2 = np.zeros((10,len(name),4))\n",
    "AUC3 = np.zeros((10,len(name),4))\n",
    "AUC4 = np.zeros((10,len(name),4))\n",
    "\n",
    "sensitivitycd,sensitivitycd2 = pickle.load(open('paper_scripts/ariamicollection_permutation_sensitivities_extra_darmanis_permute2.p','rb'))\n",
    "sensitivitycd = sensitivitycd.flatten()\n",
    "sensitivitycd2 = sensitivitycd2.flatten()\n",
    "\n",
    "fig,axs = plt.subplots(nrows=5,ncols=1)\n",
    "fig.set_size_inches(mm2inch(42.4*2,106.214))\n",
    "\n",
    "z=0\n",
    "for i in range(9):\n",
    "    S1 = sensitivitycd\n",
    "    S1e = sensitivitycd2\n",
    "    \n",
    "    for M in range(10):\n",
    "        A1 = ARIsamL[i][1][M,:]#.mean(0)\n",
    "        A2 = ARIseurL[i][1][M,:]#.mean(0)\n",
    "        A3 = ARIseurR[i][1][M,:]#.mean(0)\n",
    "        A4 = ARIseurO[i][1][M,:]\n",
    "\n",
    "        B1 = ARIsamL[i][-1][0,:][M,:]#.mean(0)\n",
    "        C1 = ARIsamL[i][-1][1,:][M,:]#.mean(0)\n",
    "        D1 = ARIsamL[i][-1][2,:][M,:]#.mean(0)\n",
    "\n",
    "        B2 = ARIseurL[i][-1][0,:][M,:]#.mean(0)\n",
    "        C2 = ARIseurL[i][-1][1,:][M,:]#.mean(0)\n",
    "        D2 = ARIseurL[i][-1][2,:][M,:]#.mean(0)        \n",
    "\n",
    "        B3 = ARIseurR[i][-1][0,:][M,:]#.mean(0)\n",
    "        C3 = ARIseurR[i][-1][1,:][M,:]#.mean(0)\n",
    "        D3 = ARIseurR[i][-1][2,:][M,:]#.mean(0)  \n",
    "        \n",
    "        B4 = ARIseurO[i][-1][0,:][M,:]#.mean(0)\n",
    "        C4 = ARIseurO[i][-1][1,:][M,:]#.mean(0)\n",
    "        D4 = ARIseurO[i][-1][2,:][M,:]#.mean(0)\n",
    "\n",
    "        AUC1[M,i,0] = met.auc(np.linspace(0,1,50),A1)\n",
    "        AUC1[M,i,1] = met.auc(np.linspace(0,1,50),A2)\n",
    "        AUC1[M,i,2] = met.auc(np.linspace(0,1,50),A3)\n",
    "        AUC1[M,i,3] = met.auc(np.linspace(0,1,50),A4)\n",
    "\n",
    "        AUC2[M,i,0] = met.auc(np.linspace(0,1,50),B1)\n",
    "        AUC2[M,i,1] = met.auc(np.linspace(0,1,50),B2)\n",
    "        AUC2[M,i,2] = met.auc(np.linspace(0,1,50),B3)\n",
    "        AUC2[M,i,3] = met.auc(np.linspace(0,1,50),B4)\n",
    "\n",
    "        AUC3[M,i,0] = met.auc(np.linspace(0,1,50),C1)\n",
    "        AUC3[M,i,1] = met.auc(np.linspace(0,1,50),C2)\n",
    "        AUC3[M,i,2] = met.auc(np.linspace(0,1,50),C3)\n",
    "        AUC3[M,i,3] = met.auc(np.linspace(0,1,50),C4)\n",
    "\n",
    "        AUC4[M,i,0] = met.auc(np.linspace(0,1,50),D1)\n",
    "        AUC4[M,i,1] = met.auc(np.linspace(0,1,50),D2)\n",
    "        AUC4[M,i,2] = met.auc(np.linspace(0,1,50),D3)\n",
    "        AUC4[M,i,3] = met.auc(np.linspace(0,1,50),D4)\n",
    "    \n",
    "    ms = 1\n",
    "    if name[i] =='Darmanis':# or name[i] == 'Baron2' or name[i] == 'Koh' or name[i] == 'Baron3':# or name[i] == 'Baron4':\n",
    "        c2='#9481c4'\n",
    "        c3='black'\n",
    "        x = np.linspace(0,1.0,A1.size)\n",
    "        A1 = ARIsamL[i][1].mean(0)\n",
    "        A2 = ARIseurL[i][1].mean(0)\n",
    "        A3 = ARIseurR[i][1].mean(0)\n",
    "        A4 = ARIseurO[i][1].mean(0)\n",
    "\n",
    "        B1 = ARIsamL[i][-1][0,:].mean(0)\n",
    "        C1 = ARIsamL[i][-1][1,:].mean(0)\n",
    "        D1 = ARIsamL[i][-1][2,:].mean(0)\n",
    "\n",
    "        B2 = ARIseurL[i][-1][0,:].mean(0)\n",
    "        C2 = ARIseurL[i][-1][1,:].mean(0)\n",
    "        D2 = ARIseurL[i][-1][2,:].mean(0)\n",
    "\n",
    "        B3 = ARIseurR[i][-1][0,:].mean(0)\n",
    "        C3 = ARIseurR[i][-1][1,:].mean(0)\n",
    "        D3 = ARIseurR[i][-1][2,:].mean(0)\n",
    "        \n",
    "        B4 = ARIseurO[i][-1][0,:].mean(0)\n",
    "        C4 = ARIseurO[i][-1][1,:].mean(0)\n",
    "        D4 = ARIseurO[i][-1][2,:].mean(0)\n",
    "        \n",
    "        A1s = ARIsamL[i][1].std(0)\n",
    "        A2s = ARIseurL[i][1].std(0)\n",
    "        A3s = ARIseurR[i][1].std(0)\n",
    "        A4s = ARIseurO[i][1].std(0)\n",
    "\n",
    "        B1s = ARIsamL[i][-1][0,:].std(0)\n",
    "        C1s = ARIsamL[i][-1][1,:].std(0)\n",
    "        D1s = ARIsamL[i][-1][2,:].std(0)\n",
    "\n",
    "        B2s = ARIseurL[i][-1][0,:].std(0)\n",
    "        C2s = ARIseurL[i][-1][1,:].std(0)\n",
    "        D2s = ARIseurL[i][-1][2,:].std(0)\n",
    "\n",
    "        B3s = ARIseurR[i][-1][0,:].std(0)\n",
    "        C3s = ARIseurR[i][-1][1,:].std(0)\n",
    "        D3s = ARIseurR[i][-1][2,:].std(0)        \n",
    "\n",
    "        B4s = ARIseurO[i][-1][0,:].std(0)\n",
    "        C4s = ARIseurO[i][-1][1,:].std(0)\n",
    "        D4s = ARIseurO[i][-1][2,:].std(0)   \n",
    "        \n",
    "        axs[1].errorbar(x,A1,yerr = A1s,color = 'blue',marker='.',linewidth=0.5,markersize=ms)        \n",
    "        axs[1].errorbar(x,A2,yerr = A2s,color = 'red',marker='.',linewidth=0.5,markersize=ms)        \n",
    "        axs[1].errorbar(x,A3,yerr = A3s,color = c2,marker='.',linewidth=0.5,markersize=ms)\n",
    "        axs[1].errorbar(x,A4,yerr = A4s,color = c3,marker='.',linewidth=0.5,markersize=ms)\n",
    "        \n",
    "        axs[0].tick_params(pad=1)\n",
    "        axs[1].tick_params(pad=1)\n",
    "        axs[2].tick_params(pad=1)\n",
    "        axs[3].tick_params(pad=1)\n",
    "        axs[4].tick_params(pad=1)\n",
    "        \n",
    "        axs[1].set_ylabel('ARI',fontsize=7)\n",
    "        axs[1].set_xticks([])\n",
    "        axs[1].set_yticks([0,0.5,1.0])\n",
    "        f=axs[1].get_xticklabels()\n",
    "        f2=axs[1].get_yticklabels()\n",
    "        for ii in f: ii.set_fontsize(7)\n",
    "        for ii in f2: ii.set_fontsize(7)                \n",
    "        \n",
    "        axs[0].set_title(name[i],fontsize=8)\n",
    "\n",
    "        axs[2].errorbar(x,B1,yerr = B1s,color = 'blue',marker='.',linewidth=0.5,markersize=ms)    \n",
    "        axs[2].errorbar(x,B2,yerr = B2s,color = 'red',marker='.',linewidth=0.5,markersize=ms)\n",
    "        axs[2].errorbar(x,B3,yerr = B3s,color = c2,marker='.',linewidth=0.5,markersize=ms)\n",
    "        axs[2].errorbar(x,B4,yerr = B4s,color = c3,marker='.',linewidth=0.5,markersize=ms)\n",
    "        \n",
    "        axs[2].set_ylabel('NACC',fontsize=7)\n",
    "        axs[2].set_xticks([])\n",
    "        \n",
    "        f=axs[2].get_xticklabels()\n",
    "        f2=axs[2].get_yticklabels()\n",
    "        for ii in f: ii.set_fontsize(7)\n",
    "        for ii in f2: ii.set_fontsize(7)    \n",
    "            \n",
    "        axs[3].errorbar(x,C1,yerr = C1s,color = 'blue',marker='.',linewidth=0.5,markersize=ms)    \n",
    "        axs[3].errorbar(x,C2,yerr = C2s,color = 'red',marker='.',linewidth=0.5,markersize=ms)\n",
    "        axs[3].errorbar(x,C3,yerr = C3s,color = c2,marker='.',linewidth=0.5,markersize=ms)  \n",
    "        axs[3].errorbar(x,C4,yerr = C4s,color = c3,marker='.',linewidth=0.5,markersize=ms)  \n",
    "        \n",
    "        axs[3].set_ylabel('Modularity',fontsize=7)\n",
    "        axs[3].set_xticks([])\n",
    "       \n",
    "        f=axs[3].get_xticklabels()\n",
    "        f2=axs[3].get_yticklabels()\n",
    "        for ii in f: ii.set_fontsize(7)\n",
    "        for ii in f2: ii.set_fontsize(7)    \n",
    "            \n",
    "        #plt.savefig('paper_scripts/FIGURE4/'+name[i]+'_C.pdf')\n",
    "        \n",
    "        #plt.figure(figsize=mm2inch(36,21));\n",
    "        axs[4].errorbar(x,D1,yerr = D1s,color = 'blue',marker='.',linewidth=0.5,markersize=ms)    \n",
    "        axs[4].errorbar(x,D2,yerr = D2s,color = 'red',marker='.',linewidth=0.5,markersize=ms)\n",
    "        axs[4].errorbar(x,D3,yerr = D3s,color = c2,marker='.',linewidth=0.5,markersize=ms)         \n",
    "        axs[4].errorbar(x,D4,yerr = D4s,color = c3,marker='.',linewidth=0.5,markersize=ms) \n",
    "      \n",
    "        axs[4].set_ylabel('| Dispersion |',fontsize=7)\n",
    "        f=axs[4].get_xticklabels()\n",
    "        f2=axs[4].get_yticklabels()\n",
    "        for ii in f: ii.set_fontsize(7)\n",
    "        for ii in f2: ii.set_fontsize(7)    \n",
    "        \n",
    "        axs[0].errorbar(x,S1,yerr=S1e,color='black',marker='.',linewidth=0.5,markersize=ms)\n",
    "        axs[4].set_xlabel('Corruption',fontsize=7)\n",
    "        axs[0].set_ylabel('Sensitivity',fontsize=7)\n",
    "        axs[4].set_xticks([0,0.5,1.0])   \n",
    "        axs[0].set_xticks([])        \n",
    "        \n",
    "        f=axs[0].get_xticklabels()\n",
    "        f2=axs[0].get_yticklabels()\n",
    "        for ii in f: ii.set_fontsize(7)\n",
    "        for ii in f2: ii.set_fontsize(7)            \n",
    "            \n",
    "        fig.subplots_adjust(wspace=0.15,hspace=0.2,left=0.3,right=1,top=0.94,bottom=0.06)\n",
    "        fig.align_ylabels(axs[:])\n",
    "        plt.savefig('/media/storage/dbox/Dropbox/paper_scripts/FIGURE4_fixed/corruptions_seurO_final.pdf',transparent=True)\n",
    "\n",
    "#plt.savefig('paper_scripts/FIGURE4/corruptions.pdf')\n",
    "#plt.savefig('/media/storage/dbox/Dropbox/paper_scripts/FIGURE4_fixed/corruptions2.pdf',transparent=True)\n",
    "        \n",
    "AUC1pm = pd.DataFrame(data=AUC1.mean(0),index=name,columns = ['SAM','Seurat','Seurat rescued','Seurat optimized'])        \n",
    "AUC2pm = pd.DataFrame(data=AUC2.mean(0),index=name,columns = ['SAM','Seurat','Seurat rescued','Seurat optimized'])\n",
    "AUC3pm = pd.DataFrame(data=AUC3.mean(0),index=name,columns = ['SAM','Seurat','Seurat rescued','Seurat optimized'])\n",
    "AUC4pm = pd.DataFrame(data=AUC4.mean(0),index=name,columns = ['SAM','Seurat','Seurat rescued','Seurat optimized'])\n",
    "        \n",
    "AUC1ps = pd.DataFrame(data=AUC1.std(0),index=name,columns = ['SAM','Seurat','Seurat rescued','Seurat optimized'])        \n",
    "AUC2ps = pd.DataFrame(data=AUC2.std(0),index=name,columns = ['SAM','Seurat','Seurat rescued','Seurat optimized'])\n",
    "AUC3ps = pd.DataFrame(data=AUC3.std(0),index=name,columns = ['SAM','Seurat','Seurat rescued','Seurat optimized'])\n",
    "AUC4ps = pd.DataFrame(data=AUC4.std(0),index=name,columns = ['SAM','Seurat','Seurat rescued','Seurat optimized'])\n",
    "#I=\n",
    "#plt.figure(); plt.scatter(s2[0,I,:,:].flatten(),ss2[I,:,:].flatten()); plt.xlabel('NACC'); plt.ylabel('ARI')\n",
    "#plt.figure(); plt.scatter(s2[1,I,:,:].flatten(),ss2[I,:,:].flatten()); plt.xlabel('Modularity'); plt.ylabel('ARI')\n",
    "#plt.figure(); plt.scatter(s2[2,I,:,:].flatten(),ss2[I,:,:].flatten()); plt.xlabel('l2disp'); plt.ylabel('ARI')\n",
    "#sam.save_figures('ami_metric_scatter.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(nrows=1,ncols=4)\n",
    "fig.set_size_inches(mm2inch(85,102.5))\n",
    "\n",
    "colors = ['black','#9481c4','red','blue']\n",
    "auc = [AUC1pm,AUC2pm,AUC3pm,AUC4pm]\n",
    "aucs = [AUC1ps,AUC2ps,AUC3ps,AUC4ps]\n",
    "nm = ['ARI','NACC','Modularity','|Dispersion|']\n",
    "\n",
    "for I in range(len(axs)):\n",
    "    f=axs[I].get_xticklabels()\n",
    "    f2=axs[I].get_yticklabels()\n",
    "    for ii in f: ii.set_fontsize(7)\n",
    "    for ii in f2: ii.set_fontsize(7)    \n",
    "    axs[I].tick_params(pad=1)\n",
    "        \n",
    "for I in range(4):\n",
    "    au = auc[I].copy()\n",
    "    aus = aucs[I].copy()\n",
    "    \n",
    "    au = au.T[['Darmanis','Wang','Segerstolpe','Muraro','Koh','Baron1','Baron2','Baron3','Baron4']].T\n",
    "    au = au.iloc[:,::-1]\n",
    "    au = au.iloc[::-1,:]\n",
    "\n",
    "    aus = aus.T[['Darmanis','Wang','Segerstolpe','Muraro','Koh','Baron1','Baron2','Baron3','Baron4']].T\n",
    "    aus = aus.iloc[:,::-1]\n",
    "    aus = aus.iloc[::-1,:]\n",
    "    \n",
    "    barlist=au.plot.barh(ax=axs[I],linewidth=0.0,xerr=aus,error_kw=dict(ecolor='black',elinewidth=0.5))#,color = 'black')\n",
    "\n",
    "    z=0\n",
    "    for i in range(au.shape[1]):\n",
    "        c = colors[i]\n",
    "        for j in range(au.shape[0]):\n",
    "            barlist.get_children()[z+4].set_color(c)\n",
    "            z+=1\n",
    "\n",
    "    axs[I].get_legend().remove()\n",
    "    box = axs[I].get_position()\n",
    "    axs[I].set_position([box.x0,box.y0,box.width,box.height])\n",
    "    #if I == 0:\n",
    "    #    axs[I].set_ylabel('AUC',fontsize=7,fontname='Arial')\n",
    "    \n",
    "    if I > 0:\n",
    "        axs[I].set_yticks([])\n",
    "        axs[I].set_yticklabels([])\n",
    "    \n",
    "    axs[I].set_title(nm[I],fontsize=8)\n",
    "    #axs[I].set_xlabel('AUC',fontsize=7)\n",
    "    \n",
    "\n",
    "    \n",
    "han,lab = axs[-1].get_legend_handles_labels()\n",
    "lab = lab[::-1]\n",
    "han = han[::-1]\n",
    "fig.subplots_adjust(top=0.95,bottom=0.05)\n",
    "#lab,han = zip(*sorted(zip(lab,han),key = lambda t: t[0]))\n",
    "#axs[-1].legend(han,lab,loc='upper right',bbox_to_anchor=(2,0.5))#,bbox_to_anchor=(1,0.5))\n",
    "\n",
    "#plt.savefig('paper_scripts/FIGURE4/AUC_corruption.pdf')\n",
    "plt.savefig('/media/storage/dbox/Dropbox/paper_scripts/FIGURE4_fixed/AUC_corruptionO.pdf',transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]\n",
    "import utilities_full as ut2\n",
    "\n",
    "NTRIALS=50\n",
    "\n",
    "funcs = [DARMANIS]#,human1,human2,human3,human4,KOH,GOLDSTAND]\n",
    "sensitivitycd=np.zeros((len(funcs),NTRIALS))\n",
    "sensitivitycd2=np.zeros((len(funcs),NTRIALS))\n",
    "\n",
    "\n",
    "for i in range(len(funcs)):\n",
    "    print(i)\n",
    "    sam=funcs[i]()\n",
    "        \n",
    "    D = sam.adata.X.copy()\n",
    "    D=D.toarray()\n",
    "    idx = np.where(D.mean(0)>0)[0]\n",
    "    z = ut2.get_fano_zscore2(D[:,idx].mean(0),D[:,idx].var(0))\n",
    "    z[np.isnan(z)]=0\n",
    "    idx = idx[np.argsort(-z)][:10000]    \n",
    "    D = D[:,idx]    \n",
    "    sam.adata = sam.adata[:,idx]\n",
    "    stds = np.round(np.linspace(0,np.prod(sam.adata.shape),NTRIALS)).astype('int64')\n",
    "    \n",
    "    for mmm in range(stds.size):\n",
    "        Ds = D.copy()\n",
    "        permute2(Ds,stds[mmm])\n",
    "        sam.adata.X=sp.sparse.csr_matrix(Ds)\n",
    "        \n",
    "        \n",
    "        print(mmm)    \n",
    "    \n",
    "        DDIF=[]\n",
    "        \n",
    "        NTR=10\n",
    "        for I in range(NTR):\n",
    "            W=np.random.choice(sam.adata.shape[1],size = int(2000),replace=False)\n",
    "            #W=sam.normalizer(W)\n",
    "            g=ut.weighted_PCA(Normalizer().fit_transform(sam.adata.X[:,W].A),do_weight=False,npcs=15)[0]\n",
    "            d=ut.compute_distances(g,'correlation')\n",
    "            Nn=ut.dist_to_nn(d,20)[0]\n",
    "            #NDIF.append(Nn)\n",
    "            DDIF.append(d)\n",
    "            #WDIF.append(g)\n",
    "        print(g.shape)\n",
    "    \n",
    "        \n",
    "        xx=NTR\n",
    "        ddif=np.zeros((xx,xx))\n",
    "        for j in range(xx):\n",
    "            for z in range(xx):\n",
    "                ddif[j,z]=np.diag(ut2.generate_correlation_map(DDIF[j],DDIF[z])).mean()\n",
    "                \n",
    "        sensitivitycd[i,mmm] = np.mean(1-ddif[ddif<1])\n",
    "        sensitivitycd2[i,mmm] = np.std(1-ddif[ddif<1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump((sensitivitycd,sensitivitycd2),open('paper_scripts/ariamicollection_permutation_sensitivities_extra_darmanis_permute2.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAMGUI import SAMGUI\n",
    "from SAM import SAM\n",
    "import pandas as pd\n",
    "sam = SAM()\n",
    "sam.load('paper_scripts/FIGURE1/manifold_graph_convergence_figures.p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = sam.co\n",
    "dt = sam.umap2d\n",
    "cl = sam.cluster_labels_k\n",
    "sam=SAM(counts=(sam.sparse_data,sam.all_gene_names,sam.all_cell_names))\n",
    "sam.adata.obsm['X_umap'] = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam.adata.obs['cl']=pd.Categorical(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e63bc10b5bc4945828bcfa29c870d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Tab(children=(FigureWidget({\n",
       "    'data': [{'hoverinfo': 'text',\n",
       "              'marker': {'size'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sg = SAMGUI(sam)\n",
    "sg.SamPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([(0.49411765, 0.20784314, 0.63529412)])*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.stab.children[0].data[0].marker.colorscale = [[0.0,'rgb(237, 30, 66)'],[0.3333333333 ,'rgb(42, 84,165)'\n",
    "                                        ],[0.666666666, 'rgb(15,178,64)'],[1.0 ,'rgb(126,53,162)']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
